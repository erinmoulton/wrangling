{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #1. This paper is about data cleaning, but more specifically data tidying. The author will go into detail of how to make messy datasets tidy with a set of tools that he demonstrates with a case study.  "
      ],
      "metadata": {
        "id": "hL9LUmzRSInt"
      },
      "id": "hL9LUmzRSInt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #2. The tidy data standard is intended to offer a standard way to organize values within a dataset. This way, you will then be able to apply these steps every time you clean tidy data, making it easier to successfully analyze the data."
      ],
      "metadata": {
        "id": "NYYBf7mCTMqx"
      },
      "id": "NYYBf7mCTMqx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #3.\n",
        "-In the introduction to section 2, the first sentence refers to the fact that families are all alike in certain ways. Everyone is related, share similar features, etc. Tidy data sets are similar to families because they are going to be strucutred similarly. On the other hand, every family is different, similar to messy data sets with their own disorganization and messiness.\n",
        "-Continuing, the second sentence refers to the understanding of information and values from the dataset but the difficulty that arises from correctly undertanding these values to then organize the data accordingly."
      ],
      "metadata": {
        "id": "4hrna1YmVb7Q"
      },
      "id": "4hrna1YmVb7Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #4\n",
        "Wickham defines values as the pieces that make up a dataset, either numbers or strings. A variable contains values that quantify a single attribute or property. Lastly, an observation contains values quantified on the same unit.\n"
      ],
      "metadata": {
        "id": "j2ZjkXfYYT0O"
      },
      "id": "j2ZjkXfYYT0O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #5\n",
        "Tidy data is defined by three characteristics with each variable being a column, each observation being a row, and each type of observational unit being a table."
      ],
      "metadata": {
        "id": "hwEIMVKckoG5"
      },
      "id": "hwEIMVKckoG5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #6\n",
        "The 5 most common problems with messy data sets are: 1. Column headers being composed of values instead of the variable names 2. A column having more than one variable 3. Variables being stored in a row and also a column 4. More than one type of observational unit being stored in a single table 5. One observational unit being stored in more than one table. The data in Table 4 is messy because the columns should be labeled by their variable, income, instead of the values of the income column. Lastly, melting data is when you merge columns into rows.\n"
      ],
      "metadata": {
        "id": "-Q8i38WrkruL"
      },
      "id": "-Q8i38WrkruL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #7\n",
        "Table 11 is messy because the days (values) are labeling the columns. Year, month, and element, could also be combined with the variable, date, similar to in Table 12 where the data is melted together. Lastly, the variable, element, in Table 12 B is fully tidy because tmax and tmin are split into variables rather than values.\n"
      ],
      "metadata": {
        "id": "oStMdzb0kyuR"
      },
      "id": "oStMdzb0kyuR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: #8\n",
        "The “chicken-and egg” problem with focusing on tidy data is that as tools are changing so will certain tactics for data tidying. Instead of having a set of rules for data tidying, it's about evolving the data science world with tools and ideas."
      ],
      "metadata": {
        "id": "dTc-QMR_k3y7"
      },
      "id": "dTc-QMR_k3y7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "pZiVeRBGSis4"
      },
      "id": "pZiVeRBGSis4"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "150haWOaPvsa"
      },
      "id": "150haWOaPvsa",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: #1"
      ],
      "metadata": {
        "id": "SzCZq12pPS3z"
      },
      "id": "SzCZq12pPS3z"
    },
    {
      "cell_type": "code",
      "source": [
        "#loading df\n",
        "url= 'https://raw.githubusercontent.com/DS3001/wrangling/main/assignment/data/airbnb_hw.csv'\n",
        "df = pd.read_csv(url, low_memory=False)"
      ],
      "metadata": {
        "id": "JmtRjAYjPZfR"
      },
      "id": "JmtRjAYjPZfR",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figuring out the unique prices\n",
        "price = df['Price']\n",
        "price.unique()"
      ],
      "metadata": {
        "id": "eDpQvZijSryb"
      },
      "id": "eDpQvZijSryb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the comma from thousands to clean the formatting to match the other numbers\n",
        "price = price.str.replace(',','')\n",
        "print(price.unique())"
      ],
      "metadata": {
        "id": "CNSji1JYTLPg"
      },
      "id": "CNSji1JYTLPg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert values to numerics\n",
        "#figuring out if there is any missing values\n",
        "price = pd.to_numeric(price,errors='coerce')\n",
        "print(price.unique())\n",
        "print('Total missing: ',sum(price.isnull()))\n",
        "#I end up with zero missing values\n"
      ],
      "metadata": {
        "id": "ADpKa3RDV19Q"
      },
      "id": "ADpKa3RDV19Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: #2"
      ],
      "metadata": {
        "id": "SOoDrBKHWrcc"
      },
      "id": "SOoDrBKHWrcc"
    },
    {
      "cell_type": "code",
      "source": [
        "#load df and figure out unique Type values to see all types of shark attacks\n",
        "url = 'https://raw.githubusercontent.com/DS3001/wrangling/main/assignment/data/sharks.csv'\n",
        "df = pd.read_csv(url, low_memory= False)\n",
        "df.head()\n",
        "type = df['Type']\n",
        "type.unique()\n",
        "type.value_counts()"
      ],
      "metadata": {
        "id": "AyAlohStXKL7"
      },
      "id": "AyAlohStXKL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting all the boat related stuff together because there are typos and redundant terms\n",
        "type = type.replace(['Sea Disaster','Boat','Boating','Boatomg'],'Watercraft')\n",
        "type.value_counts()\n",
        "#nulling all other values that arent watercraft, provoked, and unprovoked\n",
        "type = type.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation']\n",
        "                    ,np.nan)\n",
        "type.value_counts()\n",
        "df['Type']= type\n",
        "del type\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FBQtLJxqEbJS"
      },
      "id": "FBQtLJxqEbJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: #3"
      ],
      "metadata": {
        "id": "IjhlRUufRkAY"
      },
      "id": "IjhlRUufRkAY"
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2WmIgGGo42nw"
      },
      "id": "2WmIgGGo42nw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "released = df['WhetherDefendantWasReleasedPretrial']\n",
        "print(released.unique())\n",
        "print(released.value_counts())\n",
        "#the values we have are 0, 1, & 9\n",
        "#replace all 9 values (missing values) with np.nan because they're the missing values\n",
        "released = released.replace(9,np.nan)\n",
        "print(released.value_counts())\n",
        "sum(released.isnull())\n",
        "df['WhetherDefendantWasReleasedPretrial'] = released\n",
        "del released\n",
        "df.head()\n",
        "#now all we have is 0,1, & NaN!"
      ],
      "metadata": {
        "id": "Fa1y-Q-mKQf0"
      },
      "id": "Fa1y-Q-mKQf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: #4"
      ],
      "metadata": {
        "id": "uBgY8MraMR0H"
      },
      "id": "uBgY8MraMR0H"
    },
    {
      "cell_type": "code",
      "source": [
        "length = df['ImposedSentenceAllChargeInContactEvent']\n",
        "print(length.unique())\n",
        "type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "print(type.unique())\n",
        "#turn all length values into numeric values and turning non=numeric to NaN\n",
        "length = pd.to_numeric(length, errors='coerce')\n",
        "length_NA = length.isnull()\n",
        "print(np.sum(length_NA),type)\n",
        "#of 23,000 values this calculation found 9,000 missing values so I am going to\n",
        "#compare when charges were dismissed"
      ],
      "metadata": {
        "id": "YZpINEmGMUqo"
      },
      "id": "YZpINEmGMUqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (pd.crosstab(length_NA, type))\n",
        "#when type is 4 that means cases were dismissed so when type=4 length is 0\n",
        "length = length.mask( type == 4, 0)\n",
        "length = length.mask( type == 9, np.nan)\n",
        "# a new missing dummy for NaN length values\n",
        "length_NA = length.isnull()\n",
        "print( pd.crosstab(length_NA, type))\n",
        "print( np.sum(length_NA))\n",
        "#missing values are now cut down to 274 vs 9k!\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = length\n",
        "del length, type\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EoffJf31O9FP"
      },
      "id": "EoffJf31O9FP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}